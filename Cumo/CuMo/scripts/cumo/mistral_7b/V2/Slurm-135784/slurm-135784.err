/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
PyTorch: setting up devices
loading configuration file /cfs/earth/scratch/benkehel/CuMo/checkpoints/CuMo-mistral-7b/config.json
Model config LlavaMistralConfig {
  "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "LlavaMistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "balance_loss_coef": 0.0,
  "bos_token_id": 1,
  "clip_smoe": false,
  "dropout": false,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "image_aspect_ratio": "pad",
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "local_rank": 0,
  "max_position_embeddings": 32768,
  "mlp_smoe": false,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_lr": null,
  "mm_projector_type": "mlp2x_gelu",
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_mistral",
  "num_attention_heads": 32,
  "num_experts": 4,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_layers": 3,
  "num_selected": 2,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_z_loss_coef": 0.0,
  "scales": [
    1,
    3
  ],
  "sliding_window": null,
  "tie_word_embeddings": false,
  "tokenizer_model_max_length": 4096,
  "tokenizer_padding_side": "right",
  "torch_dtype": "bfloat16",
  "training": true,
  "transformers_version": "4.37.2",
  "tune_mm_mlp_adapter": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

loading weights file /cfs/earth/scratch/benkehel/CuMo/checkpoints/CuMo-mistral-7b/model.safetensors.index.json
Instantiating LlavaMistralForCausalLM model under default dtype torch.bfloat16.
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Detected flash_attn version 2.4.2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Detected flash_attn version 2.4.2
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
Detected flash_attn version 2.4.2
/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /openai/clip-vit-large-patch14-336/resolve/main/config.json HTTP/1.1" 200 0
loading configuration file config.json from cache at /cfs/earth/scratch/benkehel/huggingface/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/config.json
Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 336,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "patch_size": 14,
  "projection_dim": 768,
  "transformers_version": "4.37.2"
}

DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /openai/clip-vit-large-patch14-336/resolve/main/preprocessor_config.json HTTP/1.1" 200 0
loading configuration file preprocessor_config.json from cache at /cfs/earth/scratch/benkehel/huggingface/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/preprocessor_config.json
size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 336. Converted to {'shortest_edge': 336}.
crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 336. Converted to {'height': 336, 'width': 336}.
Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:05,  2.96s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
All model checkpoint weights were used when initializing LlavaMistralForCausalLM.

All the weights of LlavaMistralForCausalLM were initialized from the model checkpoint at /cfs/earth/scratch/benkehel/CuMo/checkpoints/CuMo-mistral-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaMistralForCausalLM for predictions without further training.
loading configuration file /cfs/earth/scratch/benkehel/CuMo/checkpoints/CuMo-mistral-7b/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2
}

loading file tokenizer.model
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading file tokenizer.json
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /openai/clip-vit-large-patch14-336/resolve/main/config.json HTTP/1.1" 200 0
loading configuration file config.json from cache at /cfs/earth/scratch/benkehel/huggingface/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/config.json
Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 336,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "patch_size": 14,
  "projection_dim": 768,
  "transformers_version": "4.37.2"
}

DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /openai/clip-vit-large-patch14-336/resolve/main/preprocessor_config.json HTTP/1.1" 200 0
loading configuration file preprocessor_config.json from cache at /cfs/earth/scratch/benkehel/huggingface/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/preprocessor_config.json
size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 336. Converted to {'shortest_edge': 336}.
crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 336. Converted to {'height': 336, 'width': 336}.
Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Using auto half precision backend
Currently training with a batch size of: 2
/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/torch/utils/cpp_extension.py:28: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Using /cfs/earth/scratch/benkehel/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /cfs/earth/scratch/benkehel/torch_extensions/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Traceback (most recent call last):
  File "/cfs/earth/scratch/benkehel/CuMo/cumo/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/cfs/earth/scratch/benkehel/CuMo/cumo/train/train.py", line 1067, in train
    trainer.train()
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
    model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 305, in __init__
    self._configure_lr_scheduler(lr_scheduler)
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 884, in _configure_lr_scheduler
    lr_scheduler = self._scheduler_from_config(self.optimizer)
  File "/cfs/earth/scratch/benkehel/.conda/envs/cumoTorch21/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 933, in _scheduler_from_config
    assert hasattr(torch.optim.lr_scheduler,
AssertionError: DeepSpeed does not recognize LR scheduler WarmupCosineLR
DEBUG:filelock:Attempting to acquire lock 23456246087344 on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Lock 23456246087344 acquired on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Attempting to release lock 23456246087344 on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Lock 23456246087344 released on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Attempting to acquire lock 23456246087344 on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Lock 23456246087344 acquired on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Attempting to release lock 23456246087344 on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Lock 23456246087344 released on /cfs/earth/scratch/benkehel/.triton_cache/Fp16Matmul_4d_kernel.pickle.lock
